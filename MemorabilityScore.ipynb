from google.colab import drive
drive.mount('/content/drive/')

import os
os.chdir("/content/drive/My Drive/CA684_Assignment")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import glob

!ls

file_1= r'/content/drive/My Drive/CA684_Assignment/Dev-set/Ground-truth/ground-truth.csv'

GT=pd.read_csv(file_1)

GT.describe()

GT.head()

def caps_read(fname):
    """Load the captions into a dataframe"""
    VN = []
    CAP = []
    DF = pd.DataFrame();
    with open(fname) as f1:
        for line in f1:
            pairs = line.split()
            VN.append(pairs[0])
            CAP.append(pairs[1])
        DF['video']=VN
        DF['caption']=CAP
    return DF

path_cap = './Dev-set/Captions/dev-set_video-captions.txt'
cap_df=caps_read(path_cap)
cap_df.head()

import nltk
nltk.download('stopwords')

import string
from nltk.corpus import stopwords

# creating text pre-process function
def text_process(mess):
  """
  1. caption splitting
  2. stopwords removal
  3. conversion of string to lower case
  4. returning string
  """
  return ' '.join([word for word in mess.split('-') if word.lower() not in stopwords.words("english")]).lower()

cap_df['clean_caption']= cap_df.caption.apply(text_process)
cap_df.head()

cap_df['Caption_length']= cap_df['clean_caption'].apply(len)
cap_df.head()

nltk.download('brown')
nltk.download('punkt')
from textblob import TextBlob

def NounCount_identify(sentence):
  Blob = TextBlob(sentence)
  return len(Blob.noun_phrases)

#creating noun count column for the data frame
cap_df['NounCount'] = cap_df['clean_caption'].apply(NounCount_identify)
cap_df.head()

result = cap_df.merge(GT, on='video')
result.head()

from wordcloud import WordCloud, STOPWORDS 
import matplotlib.pyplot as plt


comment_words = ' '
stopwords = set(STOPWORDS) 
  
for value in cap_df.clean_caption: 
      
    # typecasting each value to string 
    value = str(value) 
  
    # splitting the value 
    Tokens = value.split() 
      
    # Converting each token into Lowercase 
    for i in range(len(Tokens)): 
        Tokens[i] = Tokens[i].lower() 
          
    for words in Tokens: 
      comment_words = comment_words + words + ' '
  
  
wordcloud = WordCloud(width = 800, height = 800, 
                background_color ='Black', 
                stopwords = stopwords, 
                min_font_size = 10).generate(comment_words) 
  
# plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 
  
plt.show()

#Creating the get score function to calculate the spearmann's correlation coefficient, referred from get_Score
def Get_score(Y_pred,Y_true):
    '''Calculating Spearmann"s correlation coefficient'''
    Y_pred = np.squeeze(Y_pred)
    Y_true = np.squeeze(Y_true)
    if Y_pred.shape != Y_true.shape:
        print('Input shapes don\'t match!')
    else:
        if len(Y_pred.shape) == 1:
            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})
            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)
            print('The Spearman\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])
        else:
            for ii in range(Y_pred.shape[1]):
                Get_score(Y_pred[:,ii],Y_true[:,ii])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(result[['clean_caption','Caption_length','NounCount','nb_short-term_annotations']], \
                                                    result[['short-term_memorability']].values,\
                                                    test_size=0.2, random_state=101)

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
vectorizer = CountVectorizer()
vectorizer.fit(X_train['clean_caption'])
vectorizer.fit(X_test['clean_caption'])

X_train_vec = vectorizer.transform(X_train['clean_caption'])
X_test_vec= vectorizer.transform(X_test['clean_caption'])

X_train_vec.shape

import scipy as sp
result_train = sp.sparse.hstack((X_train_vec,X_train.drop('clean_caption', axis=1).values), format='csr')
result_test = sp.sparse.hstack((X_test_vec,X_test.drop('clean_caption', axis=1).values), format='csr')

from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(result_train, y_train)
LR_pred= LR.predict(result_test)
Get_score(LR_pred, y_test)

Xl_train, Xl_test, yl_train, yl_test = train_test_split(result[['clean_caption','Caption_length','NounCount','nb_long-term_annotations']], \
                                                    result[['long-term_memorability']].values,\
                                                    test_size=0.25, random_state=101)

#applying count vector to caption
vec_l = CountVectorizer()
vec_l.fit(Xl_train['clean_caption'])
vec_l.fit(Xl_test['clean_caption'])
Xl_train_vec = vec_l.transform(Xl_train['clean_caption'])
Xl_test_vec= vec_l.transform(Xl_test['clean_caption'])

# combination of count vector with other caption features
result_train_l = sp.sparse.hstack((Xl_train_vec,Xl_train.drop('clean_caption', axis=1).values), format='csr')
result_test_l = sp.sparse.hstack((Xl_test_vec,Xl_test.drop('clean_caption', axis=1).values), format='csr')

# applying the Linear reression model
LR.fit(result_train_l, yl_train)
LR_pred= LR.predict(result_test_l)
Get_score(LR_pred, yl_test)

#conversion of count vector into tf-idf vector 
tf_vectorizer = TfidfTransformer()
train_tfidf_vec = tf_vectorizer.fit(X_train_vec).transform(X_train_vec)
test_tfidf_vec = tf_vectorizer.fit(X_test_vec).transform(X_test_vec)

# combining tf-idf vector with other caption features
result_train = sp.sparse.hstack((train_tfidf_vec,X_train.drop('clean_caption', axis=1).values), format='csr')
result_test = sp.sparse.hstack((test_tfidf_vec,X_test.drop('clean_caption', axis=1).values), format='csr')
result_train.shape

LR.fit(result_train, y_train)
LR_pred= LR.predict(result_test)
Get_score(LR_pred, y_test)

#Conversion of count vector into tf-idf vector 
tf_vectorizer_l = TfidfTransformer()
train_tfidf_vec_l = tf_vectorizer_l.fit(Xl_train_vec).transform(Xl_train_vec)
test_tfidf_vec_l = tf_vectorizer_l.fit(Xl_test_vec).transform(Xl_test_vec)

# Combination of tf-idf vector and other caption features
result_train_l = sp.sparse.hstack((train_tfidf_vec_l,Xl_train.drop('clean_caption', axis=1).values), format='csr')
result_test_l = sp.sparse.hstack((test_tfidf_vec_l,Xl_test.drop('clean_caption', axis=1).values), format='csr')

#linear model to all caption features
LR.fit(result_train_l, yl_train)
LR_pred_l= LR.predict(result_test_l)
Get_score(LR_pred_l, yl_test)

# Neural networks library Import

from keras import Sequential
from keras import layers
from keras import regularizers

#alloting the max length to input nodes, which is actually the number of result_train columns
Max_Length = result_train.shape[0]
Max_Length

NN_model = Sequential()

NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005),input_shape=(Max_Length,))) # regular densely-connected NN layer.
NN_model.add(layers.Dropout(0.5)) #To prevent overfitting: during training, Dropout consists in randomly setting a fraction rate of input units to 0 at each update ; helps prevent overfitting.
NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005)))
NN_model.add(layers.Dropout(0.5))
NN_model.add(layers.Dense(2,activation='sigmoid'))

NN_model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])

X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(result[['clean_caption','Caption_length','NounCount','nb_short-term_annotations', 'nb_long-term_annotations']], \
                                                    result[['short-term_memorability', 'long-term_memorability']].values,\
                                                    test_size=0.2, random_state=101)


# combining tf-idf vector with other caption features
result_train_NN = sp.sparse.hstack((train_tfidf_vec,X_train_NN.drop('clean_caption', axis=1).values), format='csr')
result_test_NN = sp.sparse.hstack((test_tfidf_vec,X_test_NN.drop('clean_caption', axis=1).values), format='csr')


Max_Length = result_train_NN.shape[1]

#Defining the layers and various parameters for the neural networks
NN_model = Sequential()

NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005),input_shape=(Max_Length,))) # Just your regular densely-connected NN layer.
NN_model.add(layers.Dropout(0.5)) #Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training; helps prevent overfitting.
NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.0005)))
NN_model.add(layers.Dropout(0.5))
NN_model.add(layers.Dense(2,activation='sigmoid'))

# compile the NN_model 
NN_model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])


# training the NN_model 
History_1 = NN_model.fit(result_train_NN,y_train_NN,epochs=20,validation_data=(result_test_NN,y_test_NN))

#predicting the scores for validation data
prediction_NN = NN_model.predict(result_test_NN)

Get_score(prediction_NN, y_test_NN)

!pip install pyprind

from collections import Counter
import pyprind
from keras.preprocessing.text import Tokenizer
from string import punctuation

countS = Counter()
# setting prograss tracker
p_bar = pyprind.ProgBar(len(result['clean_caption']), title='Counting word occurrences')
for i, cap in enumerate(result['clean_caption']):
    
    p_bar.update()
    countS.update(cap.split())

result.head()

print(countS)
print(len(countS))

#tokenizing the words
Token_length = len(countS)
tokenizer = Tokenizer(num_words=Token_length)

tokenizer.fit_on_texts(list(result.clean_caption.values))

one_hot_res = tokenizer.texts_to_matrix(list(result.clean_caption.values),mode='binary')

X= one_hot_res
y= result[['short-term_memorability', 'long-term_memorability']].values

#Data split
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2, random_state=101)

# adding dropout and regularizers

NN_model = Sequential()
NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.001),input_shape=(Token_length,)))
NN_model.add(layers.Dropout(0.6))
NN_model.add(layers.Dense(10,activation='relu',kernel_regularizer=regularizers.l2(0.001)))
NN_model.add(layers.Dropout(0.5))
NN_model.add(layers.Dense(2,activation='sigmoid'))

#  NN_model Compilation
NN_model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])

#  NN_model Training 
History_1 = NN_model.fit(X_train,Y_train,epochs=20,validation_data=(X_test,Y_test))

pred = NN_model.predict(X_test)
Get_score(pred, Y_test)

#Reading C3D through a function
def read_C3D(fname):
    """Scan vectors from file_1"""
    with open(fname) as f1:
        for line in f1:
            C3D =[float(item) for item in line.split()] # convert to float type, using default separator
    return C3D

#installing tqdm package
!pip install tqdm==4.28.1

from tqdm import tqdm_notebook as tqdm
from pathlib import Path

#setting C3D feature directory path
c3d_path = Path('Dev-set/C3D')
list(c3d_path.glob('*.txt'))

#Loading file name as key and features as values
c3d_feat_dict ={}

for file in tqdm (list(c3d_path.glob('*.txt'))):
  key = file.with_suffix('.webm').name
  c3d_feat_dict[key] = read_C3D(file)

c3d_feat= pd.DataFrame(c3d_feat_dict).T.reset_index()
c3d_feat= c3d_feat.rename(columns={'index':'video'})
c3d_feat.head()

result1= result.merge(c3d_feat, on='video')
result1.head()

#data split
X = result1.drop(['video','short-term_memorability','long-term_memorability','nb_long-term_annotations','caption'],axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, \
                                                    result1['short-term_memorability'],\
                                                    test_size=0.2, random_state=101)

# On cleaned caption features, applying TF-IDF vector
vectorizer1 = CountVectorizer()
vectorizer1.fit(X_train['clean_caption'])
vectorizer1.fit(X_test['clean_caption'])
X_train_vec_1 = vectorizer1.transform(X_train['clean_caption'])
X_test_vec_1= vectorizer1.transform(X_test['clean_caption'])
tfidf_train_vec_1 = TfidfTransformer().fit(X_train_vec_1).transform(X_train_vec_1)
tfidf_test_vec_1 = TfidfTransformer().fit(X_test_vec_1).transform(X_test_vec_1)


# Combination of TF-IDF vector with C3D, caption length, noun count
result_train1 = sp.sparse.hstack((tfidf_train_vec_1,X_train.drop('clean_caption', axis=1).values), format='csr')
result_test1 = sp.sparse.hstack((tfidf_test_vec_1,X_test.drop('clean_caption', axis=1).values), format='csr')
result_train1.shape

X_l = result.drop(['video','short-term_memorability','long-term_memorability','nb_short-term_annotations','caption'],axis=1)
X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_l, \
                                                    result1['long-term_memorability'],\
                                                    test_size=0.2, random_state=101)

#On cleaned Caption, applying TF-IDF vector 
vectorizer1_l = CountVectorizer()
vectorizer1_l.fit(X_train_l['clean_caption'])
vectorizer1_l.fit(X_test_l['clean_caption'])
X_train_vec_1_l = vectorizer1_l.transform(X_train_l['clean_caption'])
X_test_vec_1_l= vectorizer1_l.transform(X_test_l['clean_caption'])

tf_vectorizer1_l = TfidfTransformer()
tfidf_train_vec_1_l = tf_vectorizer1_l.fit(X_train_vec_1_l).transform(X_train_vec_1_l)
tfidf_test_vec_1_l = tf_vectorizer1_l.fit(X_test_vec_1_l).transform(X_test_vec_1_l)


# Combinaion of TF-IDF vector with C3D, caption length, noun count
result_train1_l = sp.sparse.hstack((tfidf_train_vec_1_l,X_train_l.drop('clean_caption', axis=1).values), format='csr')
result_test1_l = sp.sparse.hstack((tfidf_test_vec_1_l,X_test_l.drop('clean_caption', axis=1).values), format='csr')
result_train1_l.shape

result_test1_l.shape

LR.fit(result_train1, y_train)
LR_pred= LR.predict(result_test1)
print("For Short-Term prediction->")
Get_score(LR_pred, y_test)

print("")

# long-term
LR.fit(result_train1_l, y_train_l)
LR_pred_l= LR.predict(result_test1_l)
print("For Long-Term prediction->")
Get_score(LR_pred_l, y_test_l)

result_train_l.shape

from sklearn.svm import SVR

svr = SVR(C=38.5, epsilon=0.05)
svr.fit(result_train, y_train)
svr_pred = svr.predict(result_test)
print("")
print("Short-term prediction=> " )
Get_score(svr_pred, y_test)

print("")

svr_l = SVR(C=38.5, epsilon=0.05)
svr_l.fit(result_train_l, yl_train)
SVR_pred_K = svr_l.predict(result_test_l)
print(" For Long-term prediction-> " )
Get_score(SVR_pred_K, yl_test)

svr_1 = SVR(C=35, epsilon=0.05)

#Short-term
svr_1.fit(result_train1, y_train)
svr_pred_1 = svr_1.predict(result_test1)
print("")
print(" Short-term prediction=> " )
Get_score(svr_pred_1, y_test)

print("")

#Long-term
svr_1_l = SVR(C=35, epsilon=0.05)
svr_1_l.fit(result_train1_l, y_train_l)
svr_pred_1_l = svr_1_l.predict(result_test1_l)
print(" For Long-term prediction-> " )
Get_score(svr_pred_1_l, y_test_l)

#Data split for short-term
X = result1.drop(['video','caption','Caption_length','NounCount',   'short-term_memorability',
                'long-term_memorability','nb_long-term_annotations','clean_caption',], axis=1)
y = result1 [['short-term_memorability']]

#Data Split
X_train, X_test, y_train, y_test = train_test_split(X,y ,test_size=0.2, random_state=101)


#Data Split for long-term
X_l = result1.drop(['video','caption','Caption_length','NounCount',   'short-term_memorability',
                'long-term_memorability','nb_short-term_annotations','clean_caption',], axis=1)
y_l = result1 [['long-term_memorability']]

#Data Split
X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_l,y_l ,test_size=0.2, random_state=101)


#SVR model to get the score
svr_2 = SVR(C=10, epsilon=0.06)

#for short-term
svr_2.fit(X_train, y_train)
svr_pred_2 = svr_2.predict(X_test)
print("")
print(" For Short-term prediction=> " )
Get_score(svr_pred_2, y_test)
      
print("")

#for long-term
svr_2.fit(X_train_l, y_train_l)
svr_pred_2_l = svr_2.predict(X_test_l)
print(" For Long-term prediction-> " )
Get_score(svr_pred_2_l, y_test_l)

GT_test = pd.read_csv('Test-set/Ground-truth_test/ground_truth_template.csv')
test_cap = caps_read('./Test-set/Captions_test/test-set-1_video-captions.txt')

test_cap.head()

import string
from nltk.corpus import stopwords

# To pre-process text created a function
def process_text(mess):
  """
  1. Splitting the caption
  2. remove stopwords
  3. convert string to lower case
  4. return string
  """
  return ' '.join([word for word in mess.split('-') if word.lower() not in stopwords.words("english")]).lower()

test_cap['clean_caption']= test_cap.caption.apply(text_process)
test_cap['Caption_length']= test_cap['clean_caption'].apply(len)
test_cap['NounCount']= test_cap['clean_caption'].apply(NounCount_identify)
test_cap.head()

test_cap.head()

def remove_alpha(s):
  return int(''.join([i for i in s if (not i.isalpha() and i !='.')]))
  
test_cap['video'] = test_cap['video'].apply(remove_alpha)
test_cap.head()

test_cap_result = test_cap.merge(GT_test, on = 'video')
test_cap_result.head()

test_cap_vec = vectorizer.transform(test_cap['clean_caption']) 
test_cap_vec = tf_vectorizer.transform(test_cap_vec)
test_cap_combined = sp.sparse.hstack((test_cap_vec,test_cap_result[['Caption_length','NounCount', 'nb_short-term_annotations']].values), format='csr')

test_cap_vec_l = vec_l.transform(test_cap['clean_caption'])
test_cap_vec_l = tf_vectorizer_l.transform(test_cap_vec_l)
test_cap_combined_l = sp.sparse.hstack((test_cap_vec_l,test_cap_result[['Caption_length','NounCount', 'nb_long-term_annotations']].values), format='csr')

svr_test_pred = svr.predict(test_cap_combined)
svr_test_pred_l = svr_l.predict(test_cap_combined_l)

GT_test['short-term_memorability']= svr_test_pred
GT_test['long-term_memorability']= svr_test_pred_l

GT_test.head()

!ls

os.chdir("/content/drive/My Drive")

GT_test.to_csv('/test_round_truth_submission.csv')

"""Test for Simple Average Ensemble Approach"""

#added the model predictions and took their average
prediction = np.add(svr_pred_1,svr_pred_1_l)
final_prediction = np.divide(ka_prediction,2)

new_pred=pd.DataFrame()
new_pred['video']=GT_test['video']
new_pred['short-term_memorability'] = final_prediction[:,0]
new_pred['nb_short-term_annotations']=GT_test['nb_short-term_annotations']
new_pred['long-term_memorability'] = final_prediction[:,1]
new_pred['nb_long-term_annotations']=GT_test['nb_long-term_annotations']
new_pred.head()

